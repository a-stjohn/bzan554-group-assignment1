{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import re\n",
    "\n",
    "# specify the path to the zipped file.\n",
    "os.chdir('/mnt/c/Users/amsj1/OneDrive - University of Tennessee/2nd_year/BZAN554_deep_learning/bzan554-group-assignment1')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def parse(path):\n",
    "    \"\"\"\n",
    "    Function to read in the VERY LARGE dataset and yield it as a generator for\n",
    "    memory efficiency. Takes one argument which is the path to the file being\n",
    "    read in. This path is set abose using 'os.chdir'.\n",
    "    \"\"\"\n",
    "    with gzip.open(path, 'rb') as g:\n",
    "        for l in g:\n",
    "            yield eval(l)\n",
    "\n",
    "#######################################################################\n",
    "# Basically load in some data to make lookup tables for the sequentially\n",
    "# learning part\n",
    "#######################################################################\n",
    "X_raw = []\n",
    "Y_raw = []\n",
    "counter = 0\n",
    "for d in parse('meta_Clothing_Shoes_and_Jewelry.json.gz'):\n",
    "    counter += 1\n",
    "    X_raw.append(d['title'])\n",
    "    Y_raw.append(d['category'])\n",
    "    if counter == 100:\n",
    "        break\n",
    "\n",
    "X_words = []\n",
    "for titles in X_raw:\n",
    "    # split the titles into words and lowercase\n",
    "    words = titles.lower().split()\n",
    "    for word in words:\n",
    "        # extract out words with minimal to no symbols\n",
    "        stripped_word = re.sub('[0-9,.,#,\\-,&,;,\\',(,),/,@,+,!]', '', word)\n",
    "        X_words.append(stripped_word)\n",
    "\n",
    "X_words = np.unique(np.array(X_words))\n",
    "\n",
    "# look up for X\n",
    "lookup_X = []\n",
    "lookup_X.extend([word for word in X_words])\n",
    "\n",
    "# flatten the Y list of lists\n",
    "flat_Y = [category for subcat in Y_raw for category in subcat]\n",
    "# uniquefy it\n",
    "unique_Y = np.unique(np.array(flat_Y))\n",
    "\n",
    "# look up for Y\n",
    "lookup_Y = []\n",
    "lookup_Y.extend([category for category in unique_Y])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "#######################################################################\n",
    "# Set up the model architecture\n",
    "#######################################################################\n",
    "inputs = tf.keras.layers.Input(shape = (len(X_words),))\n",
    "hidden1 = tf.keras.layers.Dense(\n",
    "    units = 2,\n",
    "    activation = 'sigmoid',\n",
    "    name = 'hidden1'\n",
    ")(inputs)\n",
    "hidden2 = tf.keras.layers.Dense(\n",
    "    units = 2,\n",
    "    activation = 'sigmoid',\n",
    "    name = 'hidden2'\n",
    ")(hidden1)\n",
    "output = tf.keras.layers.Dense(\n",
    "    units = len(unique_Y),\n",
    "    activation = 'softmax',\n",
    "    name = 'output'\n",
    ")(hidden2)\n",
    "\n",
    "#######################################################################\n",
    "# create & compile the model\n",
    "#######################################################################\n",
    "model = tf.keras.Model(inputs = inputs, outputs = output)\n",
    "model.compile(\n",
    "    loss = 'categorical_crossentropy',\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate = 0.001)\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-21 00:13:42.260678: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: UNKNOWN ERROR (100)\n",
      "2021-09-21 00:13:42.260723: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-U49P9H1): /proc/driver/nvidia/version does not exist\n",
      "2021-09-21 00:13:42.260947: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "#######################################################################\n",
    "# Big boy model learning time\n",
    "#######################################################################\n",
    "counter = 0\n",
    "# for i in range(1000): # number of epochs\n",
    "for d in parse('meta_Clothing_Shoes_and_Jewelry.json.gz'): # instances\n",
    "    counter += 1\n",
    "    X_raw = np.array(d['title'])\n",
    "    Y_raw = np.array(d['category'])\n",
    "\n",
    "    ###############################################\n",
    "    # logic for the X hot-one-encoded stuff\n",
    "    ###############################################\n",
    "    # split the titles into words and lowercase\n",
    "    # words = word.lower().split()\n",
    "    X_words = [words for words in str(X_raw).lower().split()]\n",
    "\n",
    "    # Get final binary stuff for X\n",
    "    X_indices = np.where(np.isin(lookup_X, X_words))\n",
    "    X_final = np.zeros(len(lookup_X))\n",
    "    X_final[X_indices] = 1\n",
    "    X_final = X_final.reshape(1, len(X_final))\n",
    "\n",
    "    ###############################################\n",
    "    # logic for the Y hot-one-encoded stuff\n",
    "    ###############################################\n",
    "    Y_indices = np.where(np.isin(lookup_Y, unique_Y))\n",
    "    Y_final = np.zeros(len(lookup_Y))\n",
    "    Y_final[Y_indices] = 1\n",
    "    Y_final = Y_final.reshape(1, len(Y_final))\n",
    "\n",
    "    ###############################################\n",
    "    # Finallllyyyyy fit the freakin model\n",
    "    ###############################################\n",
    "    model.fit(x=X_final,y=Y_final,epochs=1,batch_size=1)\n",
    "\n",
    "    if counter == 100:\n",
    "        break\n",
    "\n",
    "    # break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-21 00:13:42.538317: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1/1 [==============================] - 0s 205ms/step - loss: 2091.6653\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.6846\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.6750\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2091.6589\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.6802\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.6746\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.6794\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.6921\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2091.6641\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.6797\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.6689\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.6626\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.6602\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.6929\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.6580\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.6719\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.6692\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2091.6682\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2091.6951\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2091.6675\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.6887\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2091.6899\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2091.6858\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2091.6655\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2091.6755\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.6819\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.6919\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.6719\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.6824\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2091.6709\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2091.6907\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.6692\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.6931\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.6719\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2091.6838\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.6655\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.7090\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2091.6843\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.6851\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.6846\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.6741\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2091.6980\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.6938\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.7136\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.6775\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.6917\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2091.6875\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2091.6858\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2091.6619\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2091.6892\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.6611\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.6616\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.6709\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2091.6846\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.6882\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.6863\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.6799\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2091.6768\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.6882\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2091.6646\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.7129\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.6914\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2091.6985\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2091.6887\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.6868\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2091.7095\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2091.7151\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.7019\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.6907\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.6875\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2091.7039\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.6797\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2091.7007\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2091.7090\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.6902\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2091.6992\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.7053\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.6787\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.7078\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2091.7083\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.6792\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2091.7065\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.7322\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2091.7017\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2091.7480\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2091.7151\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.7295\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2091.7163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.7285\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2091.7073\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.7034\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2091.7163\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.7085\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.7126\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2091.7131\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2091.7612\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.7117\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.7043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2091.7244\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2091.7241\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "yhat = model.predict(x=X_final)\n",
    "yhat"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.00277744, 0.00281993, 0.00265229, 0.00299162, 0.00273926,\n",
       "        0.00263211, 0.00283557, 0.00280701, 0.00266955, 0.00275117,\n",
       "        0.00279161, 0.00293741, 0.00284017, 0.00278731, 0.00270321,\n",
       "        0.00293271, 0.00288327, 0.00260372, 0.00274327, 0.00277   ,\n",
       "        0.00294831, 0.00281733, 0.0030189 , 0.00290596, 0.00269125,\n",
       "        0.0029899 , 0.00283357, 0.00276417, 0.00293417, 0.00298697,\n",
       "        0.00278153, 0.00274223, 0.00264432, 0.00264304, 0.00290258,\n",
       "        0.00300501, 0.0028368 , 0.00281739, 0.00278779, 0.0028681 ,\n",
       "        0.00274971, 0.00260692, 0.00265254, 0.00281952, 0.0029725 ,\n",
       "        0.00301883, 0.00281708, 0.00293136, 0.00268518, 0.00282762,\n",
       "        0.00260922, 0.00297631, 0.00271172, 0.00282328, 0.00267438,\n",
       "        0.00288217, 0.0028537 , 0.0026958 , 0.00285873, 0.00278753,\n",
       "        0.00277437, 0.002989  , 0.00273626, 0.002877  , 0.002806  ,\n",
       "        0.00273662, 0.00272129, 0.00286757, 0.00284113, 0.00292489,\n",
       "        0.00265789, 0.00285031, 0.00282462, 0.00274156, 0.00288753,\n",
       "        0.00273425, 0.00288053, 0.0028937 , 0.00284377, 0.00264581,\n",
       "        0.00268927, 0.0029015 , 0.00263321, 0.00281406, 0.0028011 ,\n",
       "        0.00279722, 0.00260168, 0.00268326, 0.00299109, 0.00268416,\n",
       "        0.00290495, 0.00287939, 0.00282922, 0.00279859, 0.00282976,\n",
       "        0.00291463, 0.00299831, 0.00290665, 0.00271947, 0.00303851,\n",
       "        0.00269766, 0.00289178, 0.002829  , 0.00277714, 0.002818  ,\n",
       "        0.00288755, 0.00293369, 0.00268516, 0.00277603, 0.00286694,\n",
       "        0.00281559, 0.00293533, 0.00272632, 0.0028333 , 0.0026602 ,\n",
       "        0.00266034, 0.00286623, 0.00279069, 0.00283352, 0.00262631,\n",
       "        0.00285491, 0.00285518, 0.00273602, 0.00272175, 0.00291745,\n",
       "        0.00276813, 0.00294278, 0.00285284, 0.00276623, 0.00276699,\n",
       "        0.00278072, 0.00290255, 0.00283832, 0.00285683, 0.00264841,\n",
       "        0.00296716, 0.00278352, 0.0026921 , 0.00284609, 0.00298077,\n",
       "        0.0027862 , 0.0029077 , 0.0027277 , 0.00274883, 0.00277398,\n",
       "        0.0027902 , 0.0028821 , 0.00277711, 0.00274747, 0.0028641 ,\n",
       "        0.00278308, 0.00283619, 0.00279034, 0.00281612, 0.0027812 ,\n",
       "        0.002729  , 0.00264067, 0.00290827, 0.00300862, 0.00287889,\n",
       "        0.00290191, 0.00265427, 0.00264139, 0.00278589, 0.00290564,\n",
       "        0.00262698, 0.0028245 , 0.00263534, 0.0028138 , 0.00261964,\n",
       "        0.00277654, 0.00287616, 0.00289371, 0.00285352, 0.00275714,\n",
       "        0.00268551, 0.00280406, 0.0029156 , 0.00279714, 0.00270632,\n",
       "        0.00300195, 0.00270246, 0.00279601, 0.00284306, 0.00283651,\n",
       "        0.00291941, 0.00287502, 0.0027429 , 0.00269673, 0.002751  ,\n",
       "        0.00285543, 0.00274139, 0.00281819, 0.00282847, 0.00291082,\n",
       "        0.00276389, 0.00280047, 0.00272581, 0.00276258, 0.00295051,\n",
       "        0.0030421 , 0.00260602, 0.00273355, 0.0026713 , 0.00270694,\n",
       "        0.00277975, 0.00266764, 0.00276867, 0.00278324, 0.00261765,\n",
       "        0.00278026, 0.00297733, 0.00279566, 0.00280879, 0.00278793,\n",
       "        0.00266233, 0.00271361, 0.00280846, 0.00290209, 0.00273594,\n",
       "        0.00298913, 0.00282817, 0.00288855, 0.0027872 , 0.00279603,\n",
       "        0.0030613 , 0.00278472, 0.00264454, 0.002663  , 0.00285602,\n",
       "        0.0028916 , 0.00285338, 0.00280338, 0.00280345, 0.00303625,\n",
       "        0.00283561, 0.00299383, 0.0028707 , 0.0028171 , 0.0027432 ,\n",
       "        0.00273036, 0.0027467 , 0.00285375, 0.00279102, 0.0029125 ,\n",
       "        0.0028328 , 0.00273532, 0.00275439, 0.00276676, 0.00265312,\n",
       "        0.00280842, 0.00284851, 0.00272986, 0.00276961, 0.00273577,\n",
       "        0.00267438, 0.00265784, 0.00292011, 0.00273949, 0.00274372,\n",
       "        0.00289304, 0.00304725, 0.00267102, 0.00283796, 0.00293745,\n",
       "        0.00279458, 0.00281063, 0.00289424, 0.00291345, 0.0029164 ,\n",
       "        0.00294046, 0.00286367, 0.00298635, 0.00281513, 0.00288694,\n",
       "        0.0027498 , 0.002854  , 0.00290511, 0.00302484, 0.00283823,\n",
       "        0.00271453, 0.00301756, 0.00270275, 0.00300794, 0.00282363,\n",
       "        0.00271663, 0.00276543, 0.00286341, 0.00273359, 0.0027206 ,\n",
       "        0.00277266, 0.00268589, 0.00284136, 0.00292112, 0.00297337,\n",
       "        0.00299026, 0.00266544, 0.0027962 , 0.00281288, 0.00278484,\n",
       "        0.00293315, 0.00272256, 0.00284627, 0.00268253, 0.00265772,\n",
       "        0.00284859, 0.00284372, 0.0029287 , 0.00277714, 0.00265243,\n",
       "        0.00263681, 0.00292371, 0.0026777 , 0.00273862, 0.0028338 ,\n",
       "        0.00274084, 0.00293342, 0.00269013, 0.002607  , 0.00264717,\n",
       "        0.00273794, 0.00293627, 0.00300444, 0.00271763, 0.00267815,\n",
       "        0.00290546, 0.00279488, 0.00276139, 0.00269925, 0.00292993,\n",
       "        0.00267996, 0.00270255, 0.00274276, 0.00270437, 0.00296457,\n",
       "        0.00282508, 0.00286309, 0.00299651, 0.00281641, 0.00293852,\n",
       "        0.00272474, 0.00286242, 0.00298903, 0.00293341, 0.00288826,\n",
       "        0.00279782, 0.00267055, 0.00299484, 0.0028005 , 0.00272635,\n",
       "        0.0026565 , 0.00277207, 0.00302186, 0.00267934, 0.00267793,\n",
       "        0.00293469]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}