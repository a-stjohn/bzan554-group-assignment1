{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import re\n",
    "\n",
    "# specify the path to the zipped file.\n",
    "os.chdir('/mnt/c/Users/amsj1/OneDrive - University of Tennessee/2nd_year/BZAN554_deep_learning/bzan554-group-assignment1')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-21 19:33:06.302789: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-09-21 19:33:06.302827: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def parse(path):\n",
    "    \"\"\"\n",
    "    Function to read in the VERY LARGE dataset and yield it as a generator for\n",
    "    memory efficiency. Takes one argument which is the path to the file being\n",
    "    read in. This path is set abose using 'os.chdir'.\n",
    "    \"\"\"\n",
    "    with gzip.open(path, 'rb') as g:\n",
    "        for l in g:\n",
    "            yield eval(l)\n",
    "\n",
    "#######################################################################\n",
    "# Basically load in some data to make lookup tables for the sequentially\n",
    "# learning part\n",
    "#######################################################################\n",
    "# X_raw = []\n",
    "lookup_X = []\n",
    "Y_raw = []\n",
    "counter = 0\n",
    "for d in parse('meta_Clothing_Shoes_and_Jewelry.json.gz'):\n",
    "    counter += 1\n",
    "    # X_raw.append(d['title'])\n",
    "    X_raw = np.array(d['title'])\n",
    "    Y_raw.append(d['category'])\n",
    "\n",
    "    X_words = [words for words in str(X_raw).lower().split()]\n",
    "    lookup_X.extend(X_words)\n",
    "\n",
    "    if counter == 1000:\n",
    "        break\n",
    "\n",
    "# X_words = []\n",
    "# for titles in X_raw:\n",
    "#     # split the titles into words and lowercase\n",
    "#     words = titles.lower().split()\n",
    "#     for word in words:\n",
    "#         # extract out words with minimal to no symbols\n",
    "#         stripped_word = re.sub('[0-9,.,#,\\-,&,;,\\',(,),/,@,+,!]', '', word)\n",
    "#         X_words.append(stripped_word)\n",
    "\n",
    "# X_words = np.unique(np.array(X_words))\n",
    "\n",
    "# look up for X\n",
    "# lookup_X = []\n",
    "# lookup_X.extend([word for word in X_words])\n",
    "\n",
    "# flatten the Y list of lists\n",
    "flat_Y = [category for subcat in Y_raw for category in subcat]\n",
    "# uniquefy it\n",
    "unique_Y = np.unique(np.array(flat_Y))\n",
    "\n",
    "# look up for Y\n",
    "lookup_Y = []\n",
    "lookup_Y.extend([category for category in unique_Y])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "len(unique_Y)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1705"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#######################################################################\n",
    "# Set up the model architecture\n",
    "#######################################################################\n",
    "inputs = tf.keras.layers.Input(shape = (len(lookup_X),))\n",
    "hidden1 = tf.keras.layers.Dense(\n",
    "    units = len(lookup_X),\n",
    "    activation = 'sigmoid',\n",
    "    name = 'hidden1'\n",
    ")(inputs)\n",
    "hidden2 = tf.keras.layers.Dense(\n",
    "    units = len(lookup_Y),\n",
    "    activation = 'sigmoid',\n",
    "    name = 'hidden2'\n",
    ")(hidden1)\n",
    "output = tf.keras.layers.Dense(\n",
    "    units = len(unique_Y),\n",
    "    activation = 'sigmoid',\n",
    "    name = 'output'\n",
    ")(hidden2)\n",
    "\n",
    "#######################################################################\n",
    "# create & compile the model\n",
    "#######################################################################\n",
    "model = tf.keras.Model(inputs = inputs, outputs = output)\n",
    "model.compile(\n",
    "    loss = 'binary_crossentropy',\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate = 0.001)\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-21 19:33:07.979463: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: UNKNOWN ERROR (100)\n",
      "2021-09-21 19:33:07.979537: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-U49P9H1): /proc/driver/nvidia/version does not exist\n",
      "2021-09-21 19:33:07.979849: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#######################################################################\n",
    "# Big boy model learning time\n",
    "#######################################################################\n",
    "for i in range(10): # number of epochs\n",
    "    print(f'Epoch number: {i}')\n",
    "    counter = 0\n",
    "    for d in parse('meta_Clothing_Shoes_and_Jewelry.json.gz'): # instances\n",
    "        counter += 1\n",
    "        X_raw = np.array(d['title'])\n",
    "        Y_raw = np.array(d['category'])\n",
    "\n",
    "        ###############################################\n",
    "        # logic for the X hot-one-encoded stuff\n",
    "        ###############################################\n",
    "        # split the titles into words and lowercase\n",
    "        # words = word.lower().split()\n",
    "        X_words = [words for words in str(X_raw).lower().split()]\n",
    "\n",
    "        # Get final binary stuff for X\n",
    "        X_indices = np.where(np.isin(lookup_X, X_words))\n",
    "        X_final = np.zeros(len(lookup_X))\n",
    "        X_final[X_indices] = 1\n",
    "        X_final = X_final.reshape(1, len(X_final))\n",
    "\n",
    "        ###############################################\n",
    "        # logic for the Y hot-one-encoded stuff\n",
    "        ###############################################\n",
    "        Y_indices = np.where(np.isin(lookup_Y, unique_Y))\n",
    "        Y_final = np.zeros(len(lookup_Y))\n",
    "        Y_final[Y_indices] = 1\n",
    "        Y_final = Y_final.reshape(1, len(Y_final))\n",
    "\n",
    "        ###############################################\n",
    "        # Finallllyyyyy fit the freakin model\n",
    "        ###############################################\n",
    "        model.fit(x=X_final,y=Y_final,epochs=1,batch_size=1)\n",
    "\n",
    "        if counter == 1000:\n",
    "            break"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# save the model in folder called saved_model/my_model\n",
    "# model.save('./saved_model/my_model')\n",
    "\n",
    "# Here is how you load in the model so you don't have to re-run the fit\n",
    "# everytime\n",
    "# loaded_model = tf.keras.models.load_model('saved_model/my_model')\n",
    "# Check its architecture\n",
    "# loaded_model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 7854)]            0         \n",
      "_________________________________________________________________\n",
      "hidden1 (Dense)              (None, 7854)              61693170  \n",
      "_________________________________________________________________\n",
      "hidden2 (Dense)              (None, 1705)              13392775  \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1705)              2908730   \n",
      "=================================================================\n",
      "Total params: 77,994,675\n",
      "Trainable params: 77,994,675\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "yhat = model.predict(x=X_final)\n",
    "yhat.min()\n",
    "\n",
    "yhat_copy = yhat.copy()\n",
    "\n",
    "result_ind = yhat_copy.argsort()[0][-5:]\n",
    "result_ind"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}