{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import re\n",
    "\n",
    "# specify the path to the zipped file.\n",
    "os.chdir('/mnt/c/Users/amsj1/OneDrive - University of Tennessee/2nd_year/BZAN554_deep_learning/bzan554-group-assignment1')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def parse(path):\n",
    "    \"\"\"\n",
    "    Function to read in the VERY LARGE dataset and yield it as a generator for\n",
    "    memory efficiency. Takes one argument which is the path to the file being\n",
    "    read in. This path is set abose using 'os.chdir'.\n",
    "    \"\"\"\n",
    "    with gzip.open(path, 'rb') as g:\n",
    "        for l in g:\n",
    "            yield eval(l)\n",
    "\n",
    "#######################################################################\n",
    "# Basically load in some data to make lookup tables for the sequentially\n",
    "# learning part\n",
    "#######################################################################\n",
    "# X_raw = []\n",
    "lookup_X = []\n",
    "Y_raw = []\n",
    "counter = 0\n",
    "for d in parse('meta_Clothing_Shoes_and_Jewelry.json.gz'):\n",
    "    counter += 1\n",
    "    # X_raw.append(d['title'])\n",
    "    X_raw = np.array(d['title'])\n",
    "    Y_raw.append(d['category'])\n",
    "\n",
    "    X_words = [words for words in str(X_raw).lower().split()]\n",
    "    lookup_X.extend(X_words)\n",
    "\n",
    "    if counter == 100:\n",
    "        break\n",
    "\n",
    "# X_words = []\n",
    "# for titles in X_raw:\n",
    "#     # split the titles into words and lowercase\n",
    "#     words = titles.lower().split()\n",
    "#     for word in words:\n",
    "#         # extract out words with minimal to no symbols\n",
    "#         stripped_word = re.sub('[0-9,.,#,\\-,&,;,\\',(,),/,@,+,!]', '', word)\n",
    "#         X_words.append(stripped_word)\n",
    "\n",
    "# X_words = np.unique(np.array(X_words))\n",
    "\n",
    "# look up for X\n",
    "# lookup_X = []\n",
    "# lookup_X.extend([word for word in X_words])\n",
    "\n",
    "# flatten the Y list of lists\n",
    "flat_Y = [category for subcat in Y_raw for category in subcat]\n",
    "# uniquefy it\n",
    "unique_Y = np.unique(np.array(flat_Y))\n",
    "\n",
    "# look up for Y\n",
    "lookup_Y = []\n",
    "lookup_Y.extend([category for category in unique_Y])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "#######################################################################\n",
    "# Set up the model architecture\n",
    "#######################################################################\n",
    "inputs = tf.keras.layers.Input(shape = (len(lookup_X),))\n",
    "hidden1 = tf.keras.layers.Dense(\n",
    "    units = len(lookup_X),\n",
    "    activation = 'sigmoid',\n",
    "    name = 'hidden1'\n",
    ")(inputs)\n",
    "hidden2 = tf.keras.layers.Dense(\n",
    "    units = len(lookup_Y),\n",
    "    activation = 'sigmoid',\n",
    "    name = 'hidden2'\n",
    ")(hidden1)\n",
    "output = tf.keras.layers.Dense(\n",
    "    units = len(unique_Y),\n",
    "    activation = 'sigmoid',\n",
    "    name = 'output'\n",
    ")(hidden2)\n",
    "\n",
    "#######################################################################\n",
    "# create & compile the model\n",
    "#######################################################################\n",
    "model = tf.keras.Model(inputs = inputs, outputs = output)\n",
    "model.compile(\n",
    "    loss = 'binary_crossentropy',\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate = 0.001)\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "#######################################################################\n",
    "# Big boy model learning time\n",
    "#######################################################################\n",
    "for i in range(3): # number of epochs\n",
    "    counter = 0\n",
    "    for d in parse('meta_Clothing_Shoes_and_Jewelry.json.gz'): # instances\n",
    "        counter += 1\n",
    "        X_raw = np.array(d['title'])\n",
    "        Y_raw = np.array(d['category'])\n",
    "\n",
    "        ###############################################\n",
    "        # logic for the X hot-one-encoded stuff\n",
    "        ###############################################\n",
    "        # split the titles into words and lowercase\n",
    "        # words = word.lower().split()\n",
    "        X_words = [words for words in str(X_raw).lower().split()]\n",
    "\n",
    "        # Get final binary stuff for X\n",
    "        X_indices = np.where(np.isin(lookup_X, X_words))\n",
    "        X_final = np.zeros(len(lookup_X))\n",
    "        X_final[X_indices] = 1\n",
    "        X_final = X_final.reshape(1, len(X_final))\n",
    "\n",
    "        ###############################################\n",
    "        # logic for the Y hot-one-encoded stuff\n",
    "        ###############################################\n",
    "        Y_indices = np.where(np.isin(lookup_Y, unique_Y))\n",
    "        Y_final = np.zeros(len(lookup_Y))\n",
    "        Y_final[Y_indices] = 1\n",
    "        Y_final = Y_final.reshape(1, len(Y_final))\n",
    "\n",
    "        ###############################################\n",
    "        # Finallllyyyyy fit the freakin model\n",
    "        ###############################################\n",
    "        model.fit(x=X_final,y=Y_final,epochs=1,batch_size=1)\n",
    "\n",
    "        if counter == 100:\n",
    "            break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1/1 [==============================] - 0s 266ms/step - loss: 0.7334\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7332\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7332\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7332\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7333\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7327\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7333\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7330\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7324\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7329\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7326\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7324\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7322\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7323\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7323\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7317\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7322\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7318\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7321\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7321\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7313\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7319\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7314\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7313\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7311\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7310\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7315\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7309\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7307\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7304\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7309\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7306\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7311\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7300\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7302\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7304\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7306\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7299\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7299\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7299\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7299\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7301\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7288\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7289\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7301\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7296\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.7296\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7298\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7294\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.7299\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7295\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7290\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7295\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7291\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7288\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7285\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7286\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7288\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7282\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7292\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7283\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7287\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7287\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.7279\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.7275\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7274\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.7274\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7271\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7271\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7279\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7278\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7276\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7270\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.7264\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.7266\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7266\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.7270\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7266\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7262\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.7264\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.7264\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.7271\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7264\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7258\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7269\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7264\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7262\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7262\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7248\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7256\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7259\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7249\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7248\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7258\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7250\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7252\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.7251\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7253\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7251\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7251\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7249\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7247\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7247\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7247\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7248\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7242\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7247\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7245\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7239\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7244\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7241\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7239\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7237\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7238\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.7238\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7232\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7237\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7233\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7236\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7236\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7228\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7235\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7229\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7228\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7226\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7225\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7230\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7224\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7222\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7219\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7225\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7221\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7226\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7216\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7217\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7219\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7222\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7215\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7214\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7214\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7214\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7216\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7204\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7205\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7217\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7211\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.7211\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7214\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7209\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7214\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7210\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7206\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7211\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7207\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7203\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7201\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7202\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7204\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7198\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7207\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7198\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7202\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7203\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7195\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7191\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7189\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7189\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7187\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7187\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7194\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7193\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7192\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7186\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7180\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7182\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7182\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7186\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7182\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7178\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7180\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7180\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7187\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7180\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7174\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7185\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7180\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7179\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7178\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7164\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7172\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7175\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7165\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7165\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7174\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7166\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7168\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7167\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7169\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7167\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7168\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7165\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7163\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7163\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7163\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7164\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7158\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7164\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7161\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7155\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7160\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7157\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7155\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7153\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7154\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7155\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7149\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7154\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7149\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7153\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7153\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7145\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7151\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7146\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7144\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7143\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7142\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7147\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7140\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7139\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7136\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7142\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7138\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7143\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7132\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7134\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7136\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7139\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7132\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7131\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7131\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7131\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.7133\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7121\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7122\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.7133\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7128\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7128\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7131\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7126\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7131\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7127\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7123\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7128\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7124\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7120\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7118\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7119\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7121\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7116\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7124\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7116\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7119\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7120\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7112\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7109\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7107\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7107\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7104\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7104\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7112\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7111\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7109\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7103\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7097\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7103\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7099\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7095\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7098\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7097\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7104\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.7098\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7091\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7103\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7098\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7096\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7096\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7082\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7090\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7093\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7083\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7082\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7091\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7084\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7086\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7085\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7087\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7085\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7085\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "yhat = model.predict(x=X_final)\n",
    "yhat"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.68660796, 0.482992  , 0.48109397, 0.37659353, 0.42509547,\n",
       "        0.2899843 , 0.56060725, 0.61238474, 0.502351  , 0.42144728,\n",
       "        0.40341514, 0.27052984, 0.49330765, 0.52238065, 0.5881876 ,\n",
       "        0.55427855, 0.26908696, 0.357403  , 0.60082215, 0.43504816,\n",
       "        0.4661101 , 0.27060544, 0.5764306 , 0.57110906, 0.35426617,\n",
       "        0.39707398, 0.521375  , 0.562667  , 0.41733658, 0.46671253,\n",
       "        0.413778  , 0.4664705 , 0.6047629 , 0.46542802, 0.41201028,\n",
       "        0.70283145, 0.4171144 , 0.51953036, 0.61058056, 0.4558437 ,\n",
       "        0.64620394, 0.4029119 , 0.31916064, 0.40306658, 0.3581658 ,\n",
       "        0.5209357 , 0.3916538 , 0.58427227, 0.518163  , 0.62125915,\n",
       "        0.55668247, 0.4035258 , 0.62890285, 0.58375525, 0.42473564,\n",
       "        0.6522109 , 0.6638233 , 0.61754704, 0.3657757 , 0.53198314,\n",
       "        0.44234675, 0.5434642 , 0.54443944, 0.62236416, 0.69212687,\n",
       "        0.4628481 , 0.35777912, 0.57708126, 0.46135515, 0.5019346 ,\n",
       "        0.46750984, 0.4441597 , 0.6372826 , 0.6472943 , 0.6692865 ,\n",
       "        0.5627632 , 0.6437254 , 0.5715694 , 0.6536949 , 0.29868412,\n",
       "        0.35007513, 0.41818178, 0.4425612 , 0.4091373 , 0.5216962 ,\n",
       "        0.46651927, 0.58820814, 0.53894037, 0.7353654 , 0.4810237 ,\n",
       "        0.6039436 , 0.32545495, 0.49482253, 0.43213975, 0.643324  ,\n",
       "        0.3359444 , 0.6395453 , 0.6725626 , 0.28713292, 0.54716045,\n",
       "        0.4760601 , 0.77623165, 0.36991647, 0.4006766 , 0.4711778 ,\n",
       "        0.2551744 , 0.46863806, 0.5994003 , 0.4279783 , 0.34516406,\n",
       "        0.6092232 , 0.45498186, 0.58379495, 0.5361502 , 0.48182374,\n",
       "        0.55374783, 0.5130087 , 0.68197453, 0.6964216 , 0.25665516,\n",
       "        0.4901219 , 0.31736195, 0.5001907 , 0.534854  , 0.62703705,\n",
       "        0.5021483 , 0.50742626, 0.5354282 , 0.27871916, 0.4361327 ,\n",
       "        0.7038626 , 0.40353683, 0.6944454 , 0.6420394 , 0.62333685,\n",
       "        0.5575228 , 0.7864758 , 0.59417415, 0.63508445, 0.50879496,\n",
       "        0.6454812 , 0.5477802 , 0.32502925, 0.5589332 , 0.3683187 ,\n",
       "        0.6963196 , 0.4064318 , 0.70396644, 0.5257427 , 0.2874902 ,\n",
       "        0.5457977 , 0.59666544, 0.6152384 , 0.27729434, 0.4872937 ,\n",
       "        0.3575862 , 0.5395323 , 0.39113134, 0.54601103, 0.6529179 ,\n",
       "        0.6845026 , 0.40248278, 0.4002847 , 0.41843817, 0.59950715,\n",
       "        0.6583553 , 0.43769038, 0.62475723, 0.44316807, 0.4212615 ,\n",
       "        0.4835695 , 0.6804939 , 0.5449017 , 0.5897911 , 0.44818854,\n",
       "        0.5106867 , 0.43970013, 0.60174215, 0.6655268 , 0.4752737 ,\n",
       "        0.6018391 , 0.47281706, 0.5270241 , 0.55183893, 0.4071066 ,\n",
       "        0.48931834, 0.5559593 , 0.3633207 , 0.47808492, 0.49121612,\n",
       "        0.47553903, 0.29420903, 0.34868056, 0.61967474, 0.6465799 ,\n",
       "        0.6226469 , 0.4247353 , 0.47628757, 0.4501105 , 0.38745585,\n",
       "        0.49134168, 0.54016083, 0.4677692 , 0.42812246, 0.45931384,\n",
       "        0.4040678 , 0.35758716, 0.557098  , 0.40064937, 0.46365106,\n",
       "        0.5940013 , 0.48613828, 0.5199565 , 0.599314  , 0.2083762 ,\n",
       "        0.6094579 , 0.4857139 , 0.62931037, 0.46564597, 0.44928038,\n",
       "        0.3666975 , 0.43367273, 0.61865026, 0.64559376, 0.41335496,\n",
       "        0.7021673 , 0.481775  , 0.31279492, 0.49835455, 0.47943366,\n",
       "        0.57794636, 0.559954  , 0.5927316 , 0.6467126 , 0.57694274,\n",
       "        0.49315998, 0.5275989 , 0.6270998 , 0.5178289 , 0.5738401 ,\n",
       "        0.18829659, 0.28453285, 0.45309138, 0.60401607, 0.33024344,\n",
       "        0.51221514, 0.5515314 , 0.673983  , 0.5462472 , 0.66502655,\n",
       "        0.5528192 , 0.61303884, 0.2741933 , 0.35551822, 0.55734736,\n",
       "        0.37164444, 0.3249519 , 0.5928043 , 0.4198432 , 0.45612893,\n",
       "        0.48392323, 0.32615876, 0.54811573, 0.5457568 , 0.60248584,\n",
       "        0.6008799 , 0.3641119 , 0.35135671, 0.51602995, 0.618517  ,\n",
       "        0.4736477 , 0.46788192, 0.607054  , 0.49209487, 0.585215  ,\n",
       "        0.37302643, 0.74432683, 0.61886454, 0.45290387, 0.6511922 ,\n",
       "        0.65296245, 0.54158264, 0.57542527, 0.6481194 , 0.6627866 ,\n",
       "        0.50073963, 0.52704686, 0.29302514, 0.36794522, 0.347883  ,\n",
       "        0.7167724 , 0.5470919 , 0.5475214 , 0.51246524, 0.4962523 ,\n",
       "        0.5269713 , 0.52544856, 0.46844882, 0.47184542, 0.40022218,\n",
       "        0.4462667 , 0.6340964 , 0.5179474 , 0.375016  , 0.34099185,\n",
       "        0.6476535 , 0.47747833, 0.43903255, 0.555242  , 0.5009832 ,\n",
       "        0.18378219, 0.52802944, 0.72576416, 0.3634406 , 0.40914458,\n",
       "        0.52460164, 0.60921156, 0.56799173, 0.45040518, 0.57318294,\n",
       "        0.6405833 , 0.42728746, 0.32442015, 0.70282984, 0.5743853 ,\n",
       "        0.580381  , 0.41352925, 0.6769767 , 0.50732815, 0.5130422 ,\n",
       "        0.49297214, 0.5825624 , 0.5332476 , 0.67581856, 0.59718704,\n",
       "        0.51608807, 0.67623377, 0.551504  , 0.39704525, 0.7502628 ,\n",
       "        0.4597761 , 0.45591408, 0.29019317, 0.4666261 , 0.7479353 ,\n",
       "        0.5785768 , 0.51007146, 0.6561373 , 0.4832156 , 0.49954522,\n",
       "        0.4253235 , 0.52854764, 0.552361  , 0.50429547, 0.663061  ,\n",
       "        0.2929756 ]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}