{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import re\n",
    "\n",
    "# specify the path to the zipped file.\n",
    "os.chdir('/mnt/c/Users/amsj1/OneDrive - University of Tennessee/2nd_year/BZAN554_deep_learning/bzan554-group-assignment1')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# function creation cell\n",
    "\n",
    "def parse(path):\n",
    "    \"\"\"\n",
    "    Function to read in the VERY LARGE dataset and yield it as a generator for\n",
    "    memory efficiency. Takes one argument which is the path to the file being\n",
    "    read in. This path is set abose using 'os.chdir'.\n",
    "    \"\"\"\n",
    "    with gzip.open(path, 'rb') as g:\n",
    "        for l in g:\n",
    "            yield eval(l)\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    counter = 0\n",
    "    X = []\n",
    "    Y = []\n",
    "    for d in parse('meta_Clothing_Shoes_and_Jewelry.json.gz'):\n",
    "        counter += 1\n",
    "        X.append(d['title'])\n",
    "        Y.append(d['category'])\n",
    "        if counter == 10:\n",
    "            break\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "# get X and Y data (only 100k records)\n",
    "data_100k = get_data()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(data_100k[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#tokenize/split X (titles)\n",
    "X = []\n",
    "for titles in data_100k[0]:\n",
    "    # split the titles into words and lowercase\n",
    "    words = titles.lower().split()\n",
    "    for word in words:\n",
    "        # extract out only words with minimal to no symbols\n",
    "        stripped = re.sub('[0-9,.,#,\\-,&,;,\\',(,),/,@,+,!]', '', word)\n",
    "        X.append(stripped)\n",
    "\n",
    "X = np.array(X)\n",
    "\n",
    "# flatten, & uniquefy Y (categories)\n",
    "Y = np.array(data_100k[1])\n",
    "flat_Y = [item for subcat in Y for item in subcat]\n",
    "unique_Y = np.unique(np.array(flat_Y))\n",
    "\n",
    "# make lookup table with index value for each category\n",
    "indices = np.array(range(len(unique_Y)), dtype = np.int64)\n",
    "lookuptable = np.column_stack([unique_Y,indices])\n",
    "\n",
    "# apply lookup table to data\n",
    "main_result = []\n",
    "for i in range(len(Y)):\n",
    "    res = []\n",
    "    for ii in range(len(Y[i])):\n",
    "        res.append(int(lookuptable[lookuptable[:,0]==Y[i][ii],1][0]))\n",
    "    main_result.append(res)\n",
    "\n",
    "# create dummy encoded data\n",
    "y_final = np.array([list(np.zeros(len(unique_Y))) for i in range(len(Y))])\n",
    "for i in range(len(Y)):\n",
    "    for ii in range(len(main_result[i])):\n",
    "        y_final[i,main_result[i][ii]] = 1\n",
    "\n",
    "y_final"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# set up model architecture\n",
    "inputs = tf.keras.layers.Input(shape = (1,))\n",
    "hidden1 = tf.keras.layers.Dense(\n",
    "    units = 2,\n",
    "    activation = 'sigmoid',\n",
    "    name = 'hidden1'\n",
    ")(inputs)\n",
    "hidden2 = tf.keras.layers.Dense(\n",
    "    units = 2,\n",
    "    activation = 'sigmoid',\n",
    "    name = 'hidden2'\n",
    ")(hidden1)\n",
    "output = tf.keras.layers.Dense(\n",
    "    units = len(data_100k[1]),\n",
    "    activation = 'softmax',\n",
    "    name = 'output'\n",
    ")(hidden2)\n",
    "\n",
    "# create & compile the model\n",
    "model = tf.keras.Model(inputs = inputs, outputs = output)\n",
    "model.compile(\n",
    "    loss = 'categorical_crossentropy',\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate = 0.001)\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for i in range(1000): # number of epochs\n",
    "    for j in yield_columns(): # number of instances\n",
    "        model.fit(\n",
    "            x=j[0],\n",
    "            y=j[1],\n",
    "            epochs=1,\n",
    "            batch_size=1\n",
    "        )"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}